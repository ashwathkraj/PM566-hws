stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder((word1 + word2),n))) +
geom_col()
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder((word1,word2),n))) +
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word1,n))) +
geom_col()
mt_samples %>%
unnest_ngrams(word, input = transcription, n = 3, n_min=2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(20, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
bigrams %>%
unnest_ngrams(word, input = transcription, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10), n) %>%
bigrams %>%
unnest_ngrams(word, input = transcription, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
bigrams %>%
unnest_ngrams(word, input = transcription, n = 2, n_min = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
bigrams %>%
unnest_ngrams(word, input = abstract, n = 2, n_min = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2, n_min = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
stop_words
stop_words
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2, n_min = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word1,n))) +
geom_col()
pubmeds %>%
unnest_tokens(bigram, txt, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
top_n(15) %>%
ggplot(aes(fct_reorder(bigram, n), n)) +
geom_col() +
coord_flip() +
labs(x = NULL)
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2, n_min = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
bigrams %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word1,n))) +
geom_col()
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(word1,word2, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(token, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(token, input = abstract, n = 2, collapse = FALSE) %>%
separate (col = token, into = c("word1","word2"), sep = " ") %>%
select(word1,word2) %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(token, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(token, input = abstract, n = 2, collapse = FALSE) %>%
separate (col = token, into = c("word1","word2"), sep = " ") %>%
filter(!(word1 %in% as.character(seq(0,100)))) %>%
filter(!(word2 %in% as.character(seq(0,100)))) %>%
anti_join(
stop_words%>% select(word), by = c("word1" = "word")
) %>%
anti_join(
stop_words %>% select(word), by = c("word2" = "word")
) %>%
count(token, sort= TRUE)%>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(token,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
pubmeds %>%
unnest_ngrams(word, input = abstract, n = 2) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
pubmeds
View(pubmeds)
pubmeds %>%
unnest_tokens(token = word, input = document) %>%
anti_join(stop_words) %>%
filter(!(word %in% as.character(seq(0,100)))) %>%
count(word, sort=TRUE) %>%
top_n(10, n) %>%
ggplot(aes(x = n, y = fct_reorder(word,n))) +
geom_col()
pubmeds %>%
unnest_tokens(token = term, input = document)
pubmeds %>%
unnest_tokens(output = term, input = document)
pubmeds %>%
unnest_tokens(output = token, input = document)
pubmeds %>%
unnest_tokens(output = token, input = abstract)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(toekn, abstract)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(token, abstract)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(token, abstract)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(token, term)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n)
pubmeds %>%
unnest_tokens(output = token, input = abstract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, abtract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, abstract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, term)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(abstract, term)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(abstract, token)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(term, token)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, document)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, abstract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, abstract)%>%
count(term, abstract) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pubmeds %>%
unnest_tokens(token, abstract)%>%
count(token, term) %>%
bind_tf_idf(token, term, n) %>%
arrange(desc(tf_idf))
pub_char_list
pub_char_list
pub_char_list[2]
pub_char_list[3]
pub_char_list[1]
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2 trial vaccine',
rettype = 'abstract',
retmax= '250')
)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2+trial+vaccine',
rettype = 'abstract',
retmax= '250')
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2 trial vaccine',
rettype = 'abstract',
retmax= '250')
)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2 trial vaccine',
rettype = 'abstract',
retmax= '250')
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2 trial vaccine',
rettype = 'abstract',
retmax= 250)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
ids
]
pub_char_list[1]
str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journal <- str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journal <- str_remove_all(journal, "</?[[:alnum:]]+>")
journal <- str_replace_all(journal, "\\s+", " ")
titles
journal <- str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journal <- str_remove_all(journal, "</?[[:alnum:]]+>")
journal <- str_replace_all(journal, "\\s+", " ")
journal
```{r}
pub_char_list
pub_char_list
str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
pubdate <- str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
pubdate <- str_remove_all(journal, "</?[[:alnum:]]+>")
pubdate <- str_replace_all(journal, "\\s+", " ")
pubdate
pub_char_list
pubdate <- str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
pubdate <- str_remove_all(pubdate, "</?[[:alnum:]]+>")
pubdate <- str_replace_all(pubdate, "\\s+", " ")
pubdate
database <- data.frame(
pubmed_id = ids,
title = titles,
journal = journal,
pubdate = pubdate
abstract = abstracts
database <- data.frame(
pubmed_id = ids,
title = titles,
journal = journal,
pubdate = pubdate,
abstract = abstracts
)
knitr::opts_chunk$set(include  = FALSE)
install.packages("httr")
install.packages("xml2")
install.packages("stringr")
install.packages("rvest")
install.packages("httr")
install.packages("xml2")
install.packages("stringr")
install.packages("rvest")
knitr::opts_chunk$set(include  = TRUE)
library("httr")
library("xml2")
library("stringr")
library("rvest")
library(tidyverse)
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine+")
# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/span")
# Turning it into text
counts <- as.character(counts)
# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
library(httr)
query_ids <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
query = list(db= 'pubmed',
term = 'sars-cov2 trial vaccine',
rettype = 'abstract',
retmax= 250)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)
# Turn the result into a character vector
ids <- as.character(ids)
# Find all the ids
ids <- stringr::str_extract_all(ids, "<Id>[0-9,]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
#ids <- stringr::str_remove_all(ids, "[^0-9]")
#ids <- stringr::str_remove_all(ids, "[</Id>]")
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
# ids <- paste(ids, collapse=",")
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = ids,
retmax = 250,
rettype = "abstract"
)
)
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = ids,
retmax = 250,
rettype = "abstract"
)
)
ids2 <- ids
ids <- paste(ids, collapse=",")
publications <- GET(
url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
query = list(
db = "pubmed",
id = ids,
retmax = 250,
rettype = "abstract"
)
)
# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
pub_char_list
abstracts <- str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]]+>")
abstracts <- str_replace_all(abstracts, "\\s+", " ")
table(is.na(abstracts))
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+", " ")
titles
journal <- str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journal <- str_remove_all(journal, "</?[[:alnum:]]+>")
journal <- str_replace_all(journal, "\\s+", " ")
journal
pubdate <- str_extract(pub_char_list, "<PubDate>(\\n|.)+</PubDate>")
pubdate <- str_remove_all(pubdate, "</?[[:alnum:]]+>")
pubdate <- str_replace_all(pubdate, "\\s+", " ")
pubdate
database <- data.frame(
pubmed_id = ids,
title = titles,
journal = journal,
pubdate = pubdate,
abstract = abstracts
)
knitr::kable(database)
database <- data.frame(
pubmed_id = ids2,
title = titles,
journal = journal,
pubdate = pubdate,
abstract = abstracts
)
knitr::kable(database)
