---
title: "Assignment 3"
output: html_document
---

```{r}
knitr::opts_chunk$set(include  = FALSE)
install.packages("httr")
install.packages("xml2")
install.packages("stringr")
install.packages("rvest")
```


```{r setup}
knitr::opts_chunk$set(include  = TRUE)
library("httr")
library("xml2")
library("stringr")
library("rvest")

```


```
https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine+
```

Complete the lines of code:

```{r counter-pubmed, eval=FALSE}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine+")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")
```

```{r papers-covid-hawaii, eval=FALSE}
library(httr)
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(db= 'pubmed',
               term = 'sars-cov2 trial vaccine',
               rettype = 'abstract',
                  retmax= '250')
)

# Extracting the content of the response of GET
ids <- httr::content(query_ids)
```


```{r get-ids, eval = FALSE}
# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[0-9,]+</Id>")[[1]]

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
#ids <- stringr::str_remove_all(ids, "[^0-9]")
#ids <- stringr::str_remove_all(ids, "[</Id>]")
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
```

    
```{r get-abstracts, eval = FALSE}
ids <- paste(ids, collapse=",")

publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db = "pubmed",
    id = ids,
    retmax = 250,
    rettype = "abstract"
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)
```


```{r one-string-per-response, eval = FALSE}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```

```{r extracting-last-bit, eval = TRUE}
pub_char_list

abstracts <- str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]]+>")
abstracts <- str_replace_all(abstracts, "\\s+", " ")
table(is.na(abstracts))
```

```{r process-titles, eval = FALSE}
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+", " ")
titles
```


```{r build-db, eval = FALSE}
database <- data.frame(
  pubmed_id = ids,
  title = titles,
  name = ###
  date = ###
  abstract = abstracts
)
knitr::kable(database)
```


############ Text Mining #############

```{r}
library(readr)
pubmeds <- read_csv('https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv')

#count tokens in absracts
pubmeds %>%
  unnest_tokens(output=token, input = abstract) %>%
  count(token, sort=TRUE)

pubmeds %>%
  unnest_tokens(word, input = abstract) %>%
  anti_join(stop_words) %>%
  filter(!(word %in% as.character(seq(0,100)))) %>%
  count(word, sort=TRUE) 


pubmeds %>%
  unnest_tokens(output=token, input = abstract) %>%
  count(token, sort=TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(token,n))) + 
  geom_col()




```


